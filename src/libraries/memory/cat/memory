// -*- mode: c++ -*-
// vim: set ft=cpp:
#pragma once

#include <cat/bit>
#include <cat/simd>

namespace std {

// `address_of()` should be used instead of a `&` for containers that might
// hold a generic object, because this will produce correct results even if a
// `&` prefix operator has been overloaded.
// When this is defined in `std::`, GCC 12 can inline it for GDB.
template <typename T>
constexpr auto addressof(T& value) -> T* {
    return __builtin_addressof(value);
}

template <typename T>
constexpr auto addressof(T const&& value) -> T const* = delete;

// When this is defined in `std::`, GCC can constant-evaluate it.
// Call an in-place constructor at any address.
template <typename T, typename... Args>
constexpr auto construct_at(T* p_place, Args&&... arguments) -> decltype(auto) {
    // TODO: Assert that alignment constraints are satisfied.
    return new (const_cast<void*>(static_cast<void const volatile*>(p_place)))
        T{forward<Args>(arguments)...};
}

// Call an in-place destructor at any place.
template <typename T>
constexpr void destroy_at(T* p_place) {
    p_place->~T();
}

}  // namespace std

namespace cat {

void copy_memory(void const* p_source, void* p_destination, ssize bytes);

void copy_memory_small(void const* p_source, void* p_destination, ssize bytes);

namespace detail {
    // Type-erased `set_memory` function.
    [[gnu::optimize("-fno-tree-loop-distribute-patterns")]]
    // `tree-loop-distribute-patterns` is an optimization that replaces this
    // code with a call to `memset`. `memset` itself calls this function, which
    // makes a circular reference that ld is unable to link. Mold can link
    // this, but that optimization is useless here.
    constexpr void
    set_memory_detail(void* p_source, unsigned char byte_value, ssize bytes) {
        unsigned char* p_current_byte = static_cast<unsigned char*>(p_source);

        if (is_constant_evaluated()) {
            // Set this memory through scalar code, because `__builtin_memset()`
            // is not `constexpr` in GCC 12.
            for (ssize i = 0; i < bytes; ++i) {
                *p_current_byte = static_cast<unsigned char>(byte_value);
                ++p_current_byte;
            }
        } else {
            // Fill until `p_current_byte` has proper SIMD alignment.
            while (!is_aligned(p_current_byte,
                               NativeAbi<unsigned char>::alignment)) {
                *p_current_byte = byte_value;
                ++p_current_byte;
                --bytes;
            }

            // Non-type template parameters are not allowed in a lambda's
            // template parameter list, so the lanes are passed through an
            // `IntegralConstant`.
            auto vectorized_fill = [&]<typename LanesConstant>(LanesConstant) {
                // TODO: Consider optimizing with multiple vector loads to
                // leverage instruction-level parallelism.
                constexpr ssize lanes = LanesConstant::value;
                using Vector = FixedSizeSimd<unsigned char, lanes>;
                Vector* p_vector = bit_cast<Vector*>(p_current_byte);
                p_vector->fill(byte_value);
                p_current_byte += lanes;
                bytes -= lanes;
            };

            // Fill out 32-byte portion.
            if constexpr (NativeAbi<unsigned char>::size >= 32) {
                vectorized_fill(IntegralConstant<ssize, 32>{});
            }

            // Fill out 16-byte portion.
            if constexpr (NativeAbi<unsigned char>::size >= 16) {
                vectorized_fill(IntegralConstant<ssize, 16>{});
            }

            // Fill out remaining portion one byte at a time.
            while (bytes > 0) {
                *p_current_byte = byte_value;
                ++p_current_byte;
                --bytes;
            }
        }
    }
}  // namespace detail

// Set all bytes at this address to `byte_value`.
template <typename T = unsigned char>
    requires(sizeof(T) == 1)
constexpr void set_memory(void* p_source, T byte_value, ssize bytes) {
    detail::set_memory_detail(p_source, bit_cast<unsigned char>(byte_value),
                              bytes);
}

// `[[gnu::noinline]]` prevents GCC from constant-folding this function out.
// Set all bytes at this address to `byte_value`. This function cannot be
// constant-folded out.
template <typename T = unsigned char>
    requires(sizeof(T) == 1)
[[gnu::noinline]] constexpr void set_memory_explicit(void* p_source,
                                                     T byte_value,
                                                     ssize bytes) {
    set_memory(p_source, byte_value, bytes);
}

// Set all bytes at this address to zero.
constexpr void zero_memory(void* p_source, ssize bytes) {
    detail::set_memory_detail(p_source, static_cast<unsigned char>(0u), bytes);
}

// `[[gnu::noinline]]` prevents GCC from constant-folding this function out.
// Set all bytes at this address to zero. This function cannot be
// constant-folded out.
[[gnu::noinline]] constexpr void zero_memory_explicit(void* p_source,
                                                      ssize bytes) {
    zero_memory(p_source, bytes);
}

// Set all bytes in a trivial object to `byte_value`.
template <typename T, typename U = unsigned char>
    requires(is_trivial<T> && sizeof(U) == 1)
constexpr void set_object(T& object, U byte_value) {
    set_memory(addressof(object), byte_value, ssizeof<T>(object));
}

// Zero out all bytes in a trivial object. This function cannot be
// constant-folded out.
template <typename T, typename U = unsigned char>
    requires(is_trivial<T> && sizeof(U) == 1)
constexpr void set_object_explicit(T& object, U byte_value) {
    set_memory_explicit(addressof(object), byte_value, ssizeof<T>(object));
}

// Zero out all bytes in a trivial object.
template <typename T>
    requires(is_trivial<T>)
constexpr void zero_object(T& object) {
    zero_memory(addressof(object), ssizeof<T>(object));
}

// Zero out all bytes in a trivial object. This function cannot be
// constant-folded out.
template <typename T>
    requires(is_trivial<T>)
constexpr void zero_object_explicit(T& object) {
    zero_memory_explicit(addressof(object), ssizeof<T>(object));
}

// Prevent a compiler from constant-folding the non-`const`-qualified `p_object`
// when it shares its location in memory with, but has distinct lifetime from, a
// separate `const`-qualified object.
template <typename T>
[[nodiscard]] constexpr auto launder(T* p_object) -> T* {
    return __builtin_launder(p_object);
}

// `launder` cannot be used on functions.
template <typename FunctionPtrReturn, typename... Args>
void launder(FunctionPtrReturn (*)(Args...)) = delete;

template <typename FunctionPtrReturn, typename... Args>
void launder(FunctionPtrReturn (*)(Args..., ...)) = delete;

// `launder` cannot be used on `void*`
void launder(void*) = delete;
void launder(void const*) = delete;
void launder(void volatile*) = delete;
void launder(void const volatile*) = delete;

using std::addressof;
using std::construct_at;
using std::destroy_at;

template <typename T>
auto relocate_at(T const* p_source, T* p_destination) -> T* {
    if constexpr (is_trivially_relocatable<T>) {
        copy_memory(p_source, p_destination, ssizeof<T>());
        return launder(p_destination);
    } else {
        T* p_new = construct_at(p_destination, move(*p_source));
        p_source->~T();
        return p_new;
    }
}

template <typename T>
auto relocate(T& source, T* p_destination) -> T* {
    return relocate_at(addressof(source), p_destination);
}

}  // namespace cat

using std::addressof;
